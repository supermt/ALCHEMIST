{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600779451510",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import re\n",
    "from log_class import log_recorder\n",
    "\n",
    "LOG_DIR = \"./test_data2/\"\n",
    "report_csv = \"report.csv\"\n",
    "LOG_file = \"LOG\"\n",
    "\n",
    "\n",
    "# from log_class import log_recorder\n",
    "COMPACTION_LOG_HEAD = \"/compaction/compaction_job.cc:755\"\n",
    "FLUSH_LOG_BEGIN = \"flush_started\"\n",
    "FLUSH_LOG_END = \"flush_finished\"\n",
    "FLUSH_FILE_CREATEION = \"table_file_creation\"\n",
    "\n",
    "def load_log_and_qps(log_file, ground_truth_csv):\n",
    "    # load the data\n",
    "    return log_recorder(log_file,ground_truth_csv)\n",
    "\n",
    "data_set = load_log_and_qps(LOG_DIR+LOG_file, LOG_DIR+report_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut the timeline into pieces according to the time slice size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ms_to_sec  = 1000000\n",
    "time_slice = 100000 # 100 miusec, 100,000ms\n",
    "switch_ratio = ms_to_sec/time_slice\n",
    "\n",
    "real_time_speed=data_set.qps_df\n",
    "flush_jobs = data_set.flush_df\n",
    "# op_type, 0 for flush, 1 for compaction; \n",
    "# triggered or not, 0 for not triggered, 1 for running\n",
    "# input size, how many bytes have been read from disk\n",
    "# total_operation_bytes\n",
    "# job_id, i don't kown whether it can be used or not.\n",
    "input_tuple=[0,0,0,12345,7] \n",
    "\n",
    "# then we put all the operations into a series of input tupler stack\n",
    "#        -----\n",
    "#      ----\n",
    "# --- -- \n",
    "# -- -------\n",
    "# 1 2 3 4 5 6 7 8 9 10\n",
    "\n",
    "\n",
    "\n",
    "# create the bucket first\n",
    "\n",
    "# print(int(flush_jobs[flush_jobs[\"job\"]==11902][\"start_time\"]/time_slice))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "bucket = []\n",
    "for i in range(int(real_time_speed.tail(1)[\"microsecs_elapsed\"] * switch_ratio)):\n",
    "    bucket.append([])\n",
    "# then we use a bucket sort idea to count down the rest things\n",
    "for flush_job in data_set.flush_df.iloc():\n",
    "    # indices = (int(flush_job[\"start_time\"]/time_slice), flush_job[\"end_time\"]/time_slice)\n",
    "    start_index = int(flush_job[\"start_time\"]/time_slice)\n",
    "    end_index=int(flush_job[\"end_time\"]/time_slice) + 1\n",
    "    payload = round(flush_job[\"flush_size\"] / (1024*1024) ,2) # change to MB will be easier to calculate\n",
    "    job_id = flush_job[\"job\"]\n",
    "\n",
    "    if start_index >= len(bucket)-10 or end_index >= len(bucket)-5: # the tail part is not accurant\n",
    "        break\n",
    "    for bucket_element in bucket[start_index:end_index]:\n",
    "        bucket_element.append([0,0,payload,job_id])\n",
    "\n",
    "\n",
    "for compaction_job in data_set.compaction_df.iloc():\n",
    "    start_index = int(compaction_job[\"start_time\"]/time_slice)\n",
    "    end_index=int(compaction_job[\"end_time\"]/time_slice) + 1\n",
    "    input_size = round(compaction_job[\"input_data_size\"] / (1024*1024) ,2) # change to MB will be easier to calculate\n",
    "    output_size = round(compaction_job[\"total_output_size\"] / (1024*1024) ,2)\n",
    "    job_id = compaction_job[\"job\"]\n",
    "    if start_index >= len(bucket)-10 or end_index >= len(bucket)-5: # the tail part is not accurant\n",
    "        break\n",
    "    compaction_tuple=[1,input_size,output_size,job_id]\n",
    "    for bucket_element in bucket[start_index:end_index]:\n",
    "        bucket_element.append(compaction_tuple)\n",
    "\n",
    "result_tensor = real_time_speed[\"interval_qps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transformer model\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != src.size(0):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(src.size(0)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}