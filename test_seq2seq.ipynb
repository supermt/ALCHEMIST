{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600851770592",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f5092c0c84c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticker\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import re\n",
    "from log_class import log_recorder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "LOG_DIR = \"./test_data2/\"\n",
    "report_csv = \"report.csv\"\n",
    "LOG_file = \"LOG\"\n",
    "\n",
    "\n",
    "# from log_class import log_recorder\n",
    "COMPACTION_LOG_HEAD = \"/compaction/compaction_job.cc:755\"\n",
    "FLUSH_LOG_BEGIN = \"flush_started\"\n",
    "FLUSH_LOG_END = \"flush_finished\"\n",
    "FLUSH_FILE_CREATEION = \"table_file_creation\"\n",
    "\n",
    "def load_log_and_qps(log_file, ground_truth_csv):\n",
    "    # load the data\n",
    "    return log_recorder(log_file,ground_truth_csv)\n",
    "\n",
    "data_set = load_log_and_qps(LOG_DIR+LOG_file, LOG_DIR+report_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut the timeline into pieces according to the time slice size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'data_set' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9048f83d00a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mswitch_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mms_to_sec\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtime_slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mreal_time_speed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqps_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mflush_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# op_type, 0 for flush, 1 for compaction;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_set' is not defined"
     ]
    }
   ],
   "source": [
    "ms_to_sec  = 1000000\n",
    "time_slice = 100000 # 100 miusec, 100,000ms\n",
    "switch_ratio = ms_to_sec/time_slice\n",
    "\n",
    "real_time_speed=data_set.qps_df\n",
    "flush_jobs = data_set.flush_df\n",
    "# op_type, 0 for flush, 1 for compaction; \n",
    "# triggered or not, 0 for not triggered, 1 for running\n",
    "# input size, how many bytes have been read from disk\n",
    "# total_operation_bytes\n",
    "# job_id, i don't kown whether it can be used or not.\n",
    "input_tuple=[0,0,12345,7] \n",
    "\n",
    "# then we put all the operations into a series of input tupler stack\n",
    "#        -----\n",
    "#      ----\n",
    "# --- -- \n",
    "# -- -------\n",
    "# 1 2 3 4 5 6 7 8 9 10\n",
    "\n",
    "\n",
    "\n",
    "# create the bucket first\n",
    "\n",
    "# print(int(flush_jobs[flush_jobs[\"job\"]==11902][\"start_time\"]/time_slice))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'real_time_speed' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c4c837313245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_time_speed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"microsecs_elapsed\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mswitch_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# then we use a bucket sort idea to count down the rest things\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mflush_job\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'real_time_speed' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "bucket = []\n",
    "for i in range(int(real_time_speed.tail(1)[\"microsecs_elapsed\"] * switch_ratio)):\n",
    "    bucket.append([])\n",
    "# then we use a bucket sort idea to count down the rest things\n",
    "for flush_job in data_set.flush_df.iloc():\n",
    "    # indices = (int(flush_job[\"start_time\"]/time_slice), flush_job[\"end_time\"]/time_slice)\n",
    "    start_index = int(flush_job[\"start_time\"]/time_slice)\n",
    "    end_index=int(flush_job[\"end_time\"]/time_slice) + 1\n",
    "    payload = round(flush_job[\"flush_size\"] / (1024*1024) ,2) # change to MB will be easier to calculate\n",
    "    job_id = flush_job[\"job\"]\n",
    "\n",
    "    if start_index >= len(bucket)-10 or end_index >= len(bucket)-5: # the tail part is not accurant\n",
    "        break\n",
    "    for bucket_element in bucket[start_index:end_index]:\n",
    "        bucket_element.append([0,0,payload,job_id])\n",
    "\n",
    "\n",
    "for compaction_job in data_set.compaction_df.iloc():\n",
    "    start_index = int(compaction_job[\"start_time\"]/time_slice)\n",
    "    end_index=int(compaction_job[\"end_time\"]/time_slice) + 1\n",
    "    input_size = round(compaction_job[\"input_data_size\"] / (1024*1024) ,2) # change to MB will be easier to calculate\n",
    "    output_size = round(compaction_job[\"total_output_size\"] / (1024*1024) ,2)\n",
    "    job_id = compaction_job[\"job\"]\n",
    "    if start_index >= len(bucket)-10 or end_index >= len(bucket)-5: # the tail part is not accurant\n",
    "        break\n",
    "    compaction_tuple=[1,input_size,output_size,job_id]\n",
    "    for bucket_element in bucket[start_index:end_index]:\n",
    "        bucket_element.append(compaction_tuple)\n",
    "\n",
    "result_tensor = real_time_speed[\"interval_qps\"]\n",
    "max_length = max([len(x) for x in bucket])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "empty_tuple = [0,0,0,0]\n",
    "for element in bucket:\n",
    "    while len(element) < max_length:\n",
    "        element.append(empty_tuple)\n",
    "import torch\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'data_set' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2350d07cb8c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mqps_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqps_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"interval_qps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqps_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mqps_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_set' is not defined"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor(bucket)\n",
    "qps_list = list(data_set.qps_df[\"interval_qps\"]) \n",
    "while len(qps_list) < input_tensor.size(0):\n",
    "    qps_list.append(0)\n",
    "\n",
    "output_tensor = torch.tensor(qps_list).reshape(-1,1)\n",
    "\n",
    "print(input_tensor.size(),output_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([0])\n"
    }
   ],
   "source": [
    "flatten_input = []\n",
    "for element in bucket:\n",
    "    temp_list = []\n",
    "    for xx in element:\n",
    "        for x in xx:\n",
    "            temp_list.append(x)\n",
    "    flatten_input.append(temp_list)\n",
    "\n",
    "flatten_input_tensor = torch.tensor(flatten_input)\n",
    "# # print(flatten_input)\n",
    "print(flatten_input_tensor.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'data_set' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6f2d926e211f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtime_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqps_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"microsecs_elapsed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maggregated_qps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtime_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtime_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maggregated_qps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqps_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"interval_qps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqps_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"microsecs_elapsed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_set' is not defined"
     ]
    }
   ],
   "source": [
    "time_idx = set(data_set.qps_df[\"microsecs_elapsed\"])\n",
    "aggregated_qps = []\n",
    "for time_id in time_idx:\n",
    "    aggregated_qps.append(sum(data_set.qps_df[\"interval_qps\"][time_id == data_set.qps_df[\"microsecs_elapsed\"]]))\n",
    "\n",
    "aggregate_output_tensor = torch.tensor(aggregated_qps).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim,\n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim,\n",
    "                                                  dropout, \n",
    "                                                  device) \n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        #src = [batch size, src len, hid dim]\n",
    "            \n",
    "        return src"
   ]
  }
 ]
}