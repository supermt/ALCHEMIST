{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600844464686",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "before batchify: torch.Size([2086708, 1])\nafter batchify: torch.Size([104335, 20])\ntorch.Size([104335, 20])\ntensor([[    3,    25,  1849,  ...,     5,    65,    30],\n        [   12,    66,    13,  ...,    35,  2438,  4064],\n        [ 3852, 13667,  2962,  ...,   902,    33,    20],\n        ...,\n        [  154,     7,    10,  ...,     5,  1076,    78],\n        [   25,     4,  4135,  ...,     4,    56,   299],\n        [    6,    57,   385,  ...,  3168,   737,    36]], device='cuda:0')\ntorch.Size([35, 20])\ntorch.Size([700])\n"
    }
   ],
   "source": [
    "\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    print(\"before batchify:\",data.size())\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    print(\"after batchify:\",data.size())\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "bptt = 35\n",
    "\n",
    "def get_batch(source, i):\n",
    "    print(source.size())\n",
    "    print(source)\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "data,target = get_batch(train_data,0)\n",
    "\n",
    "print(data.size())\n",
    "print(target)"
   ]
  }
 ]
}